# Context Generation

## 1. Approaches

- Similarity search with embeddings
- Similarity search with enhanced AI-Chains
- 

## 2. Benchmarking

### 2.1 What kinds of prompts are there?

- _TODO:_ analyze search logs 
- Identifying something or somebody
    - **Example:** who is the project manager of project XYZ? Who are the developers of XYZ?)
- General summarizing questions
    - **Example:** what are the story arcs of campaign 3?
- Tutorials (Onboarding, Process How-To's, development tutorials):
    - **Example:** how do I setup the project?
    - **Example:** how do I configure the local proxy?
    - **Example:** what is the process for doing XYZ?

### 2.2 Benchmarking Process

Assistant performance will be evaluated by defining a list/prompts of questions along with the expected context pages for each data set. 
The actually retrieved context should include those expected pages. 
Data Set and expected results can be defined in JSON or YAML? Or as Java Playbook? We'll see

## 3. Data Sets

### 3.1 Pet Fellows Home Page

### 3.2 Critical Role Campaign 3

### 3.3 Atlassian Demo Instances

### 3.4 Educational Institutions

### 3.5 Open Source Wiki Platforms

### 3.6 Public Documentation Repositories