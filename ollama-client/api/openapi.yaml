openapi: 3.0.3
info:
  description: API Spec for Ollama API. Please see https://github.com/jmorganca/ollama/blob/main/docs/api.md
    for more details.
  title: Ollama API
  version: 0.1.9
servers:
- description: Ollama server URL
  url: http://localhost:11434/api
tags:
- description: "Given a prompt, the model will generate a completion."
  name: Completions
- description: "Given a list of messages comprising a conversation, the model will\
    \ return a response."
  name: Chat
- description: Get a vector representation of a given input.
  name: Embeddings
- description: List and describe the various models available.
  name: Models
paths:
  /generate:
    post:
      description: The final response object will include statistics and additional
        data from the request.
      operationId: generateCompletion
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateCompletionRequest'
      responses:
        "200":
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/GenerateCompletionResponse'
          description: Successful operation.
      summary: Generate a response for a given prompt with a provided model.
      tags:
      - Completions
      x-content-type: application/json
      x-accepts: application/x-ndjson
  /chat:
    post:
      description: "This is a streaming endpoint, so there will be a series of responses.\
        \ The final response object will include statistics and additional data from\
        \ the request."
      operationId: generateChatCompletion
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateChatCompletionRequest'
      responses:
        "200":
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/GenerateChatCompletionResponse'
          description: Successful operation.
      summary: Generate the next message in a chat with a provided model.
      tags:
      - Chat
      x-content-type: application/json
      x-accepts: application/x-ndjson
  /embeddings:
    post:
      operationId: generateEmbedding
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateEmbeddingRequest'
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateEmbeddingResponse'
          description: Successful operation.
      summary: Generate embeddings from a model.
      tags:
      - Embeddings
      x-content-type: application/json
      x-accepts: application/json
  /create:
    post:
      description: "It is recommended to set `modelfile` to the content of the Modelfile\
        \ rather than just set `path`. This is a requirement for remote create. Remote\
        \ model creation should also create any file blobs, fields such as `FROM`\
        \ and `ADAPTER`, explicitly with the server using Create a Blob and the value\
        \ to the path indicated in the response."
      operationId: createModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateModelRequest'
        description: Create a new model from a Modelfile.
      responses:
        "200":
          content:
            application/x-ndjson:
              schema:
                $ref: '#/components/schemas/CreateModelResponse'
          description: Successful operation.
      summary: Create a model from a Modelfile.
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/x-ndjson
  /tags:
    get:
      operationId: listModels
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelsResponse'
          description: Successful operation.
      summary: List models that are available locally.
      tags:
      - Models
      x-accepts: application/json
  /show:
    post:
      operationId: showModelInfo
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ModelInfoRequest'
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelInfo'
          description: Successful operation.
      summary: "Show details about a model including modelfile, template, parameters,\
        \ license, and system prompt."
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/json
  /copy:
    post:
      operationId: copyModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CopyModelRequest'
      responses:
        "200":
          description: Successful operation.
      summary: Creates a model with another name from an existing model.
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/json
  /delete:
    delete:
      operationId: deleteModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/DeleteModelRequest'
      responses:
        "200":
          description: Successful operation.
      summary: Delete a model and its data.
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/json
  /pull:
    post:
      description: "Cancelled pulls are resumed from where they left off, and multiple\
        \ calls will share the same download progress."
      operationId: pullModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/PullModelRequest'
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PullModelResponse'
          description: Successful operation.
      summary: Download a model from the ollama library.
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/json
  /push:
    post:
      description: Requires registering for ollama.ai and adding a public key first.
      operationId: pushModel
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/PushModelRequest'
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PushModelResponse'
          description: Successful operation.
      summary: Upload a model to a model library.
      tags:
      - Models
      x-content-type: application/json
      x-accepts: application/json
  /blobs/{digest}:
    head:
      operationId: checkBlob
      parameters:
      - description: the SHA256 digest of the blob
        example: sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250
        explode: true
        in: query
        name: name
        required: true
        schema:
          type: string
        style: form
      responses:
        "200":
          description: Blob exists on the server
        "404":
          description: Blob was not found
      summary: Check to see if a blob exists on the Ollama server which is useful
        when creating models.
      tags:
      - Models
      x-accepts: application/json
    post:
      operationId: createBlob
      parameters:
      - description: the SHA256 digest of the blob
        example: sha256:c8edda1f17edd2f1b60253b773d837bda7b9d249a61245931a4d7c9a8d350250
        explode: true
        in: query
        name: name
        required: true
        schema:
          type: string
        style: form
      requestBody:
        content:
          application/octet-stream:
            schema:
              format: binary
              type: string
      responses:
        "201":
          description: Blob was successfully created
      summary: Create a blob from a file. Returns the server file path.
      tags:
      - Models
      x-content-type: application/octet-stream
      x-accepts: application/json
components:
  schemas:
    GenerateCompletionRequest:
      description: Request class for the generate endpoint.
      example:
        template: template
        images:
        - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
        - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
        system: system
        stream: false
        context:
        - 0
        - 0
        options:
          vocab_only: true
          seed: 1
          mirostat: 1
          f16_kv: true
          presence_penalty: 7.386282
          num_batch: 1
          penalize_newline: true
          top_p: 2.302136
          frequency_penalty: 1.2315135
          top_k: 5
          temperature: 2.027123
          use_mmap: true
          repeat_last_n: 3
          mirostat_eta: 6.846853
          main_gpu: 9
          logits_all: true
          num_thread: 8
          low_vram: true
          numa: true
          num_predict: 5
          rope_frequency_base: 9.36931
          tfs_z: 7.0614014
          num_ctx: 7
          num_gpu: 5
          use_mlock: true
          stop:
          - stop
          - stop
          mirostat_tau: 1.4894159
          repeat_penalty: 4.145608
          embedding_only: true
          num_keep: 6
          typical_p: 9.301444
          rope_frequency_scale: 6.6835623
          num_gqa: 4
        format: null
        raw: true
        model: llama2:7b
        prompt: Why is the sky blue?
      properties:
        model:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        prompt:
          description: The prompt to generate a response.
          example: Why is the sky blue?
          type: string
        images:
          description: (optional) a list of Base64-encoded images to include in the
            message (for multimodal models such as llava)
          items:
            description: Base64-encoded image (for multimodal models such as llava)
            example: iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
            type: string
          type: array
        system:
          description: The system prompt to (overrides what is defined in the Modelfile).
          type: string
        template:
          description: The full prompt or prompt template (overrides what is defined
            in the Modelfile).
          type: string
        context:
          description: "The context parameter returned from a previous request to\
            \ [generateCompletion], this can be used to keep a short conversational\
            \ memory."
          items:
            type: integer
          type: array
        options:
          $ref: '#/components/schemas/RequestOptions'
        format:
          $ref: '#/components/schemas/ResponseFormat'
        raw:
          description: "If `true` no formatting will be applied to the prompt and\
            \ no context will be returned. \n\nYou may choose to use the `raw` parameter\
            \ if you are specifying a full templated prompt in your request to the\
            \ API, and are managing history yourself.\n"
          type: boolean
        stream:
          default: false
          description: |
            If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.
          type: boolean
      required:
      - model
      - prompt
      type: object
    RequestOptions:
      description: Additional model parameters listed in the documentation for the
        Modelfile such as `temperature`.
      example:
        vocab_only: true
        seed: 1
        mirostat: 1
        f16_kv: true
        presence_penalty: 7.386282
        num_batch: 1
        penalize_newline: true
        top_p: 2.302136
        frequency_penalty: 1.2315135
        top_k: 5
        temperature: 2.027123
        use_mmap: true
        repeat_last_n: 3
        mirostat_eta: 6.846853
        main_gpu: 9
        logits_all: true
        num_thread: 8
        low_vram: true
        numa: true
        num_predict: 5
        rope_frequency_base: 9.36931
        tfs_z: 7.0614014
        num_ctx: 7
        num_gpu: 5
        use_mlock: true
        stop:
        - stop
        - stop
        mirostat_tau: 1.4894159
        repeat_penalty: 4.145608
        embedding_only: true
        num_keep: 6
        typical_p: 9.301444
        rope_frequency_scale: 6.6835623
        num_gqa: 4
      properties:
        num_keep:
          description: |
            Number of tokens to keep from the prompt.
          type: integer
        seed:
          description: |
            Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)
          type: integer
        num_predict:
          description: |
            Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)
          type: integer
        top_k:
          description: |
            Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
          type: integer
        top_p:
          description: |
            Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
          format: float
          type: number
        tfs_z:
          description: |
            Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)
          format: float
          type: number
        typical_p:
          description: |
            Typical p is used to reduce the impact of less probable tokens from the output.
          format: float
          type: number
        repeat_last_n:
          description: |
            Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
          type: integer
        temperature:
          description: |
            The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)
          format: float
          type: number
        repeat_penalty:
          description: |
            Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
          format: float
          type: number
        presence_penalty:
          description: |
            Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
          format: float
          type: number
        frequency_penalty:
          description: |
            Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
          format: float
          type: number
        mirostat:
          description: |
            Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
          type: integer
        mirostat_tau:
          description: |
            Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)
          format: float
          type: number
        mirostat_eta:
          description: |
            Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)
          format: float
          type: number
        penalize_newline:
          description: |
            Penalize newlines in the output. (Default: false)
          type: boolean
        stop:
          description: Sequences where the API will stop generating further tokens.
            The returned text will not contain the stop sequence.
          items:
            type: string
          type: array
        numa:
          description: |
            Enable NUMA support. (Default: false)
          type: boolean
        num_ctx:
          description: |
            Sets the size of the context window used to generate the next token.
          type: integer
        num_batch:
          description: |
            Sets the number of batches to use for generation. (Default: 1)
          type: integer
        num_gqa:
          description: |
            The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for `llama2:70b`.
          type: integer
        num_gpu:
          description: |
            The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable.
          type: integer
        main_gpu:
          description: |
            The GPU to use for the main model. Default is 0.
          type: integer
        low_vram:
          description: |
            Enable low VRAM mode. (Default: false)
          type: boolean
        f16_kv:
          description: |
            Enable f16 key/value. (Default: false)
          type: boolean
        logits_all:
          description: |
            Enable logits all. (Default: false)
          type: boolean
        vocab_only:
          description: |
            Enable vocab only. (Default: false)
          type: boolean
        use_mmap:
          description: |
            Enable mmap. (Default: false)
          type: boolean
        use_mlock:
          description: |
            Enable mlock. (Default: false)
          type: boolean
        embedding_only:
          description: |
            Enable embedding only. (Default: false)
          type: boolean
        rope_frequency_base:
          description: |
            The base of the rope frequency scale. (Default: 1.0)
          format: float
          type: number
        rope_frequency_scale:
          description: |
            The scale of the rope frequency. (Default: 1.0)
          format: float
          type: number
        num_thread:
          description: |
            Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores).
          type: integer
      type: object
    ResponseFormat:
      description: |
        The format to return a response in. Currently the only accepted value is json.

        Enable JSON mode by setting the format parameter to json. This will structure the response as valid JSON.

        Note: it's important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.
      enum:
      - json
      type: string
    GenerateCompletionResponse:
      description: The response class for the generate endpoint.
      properties:
        model:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        created_at:
          description: Date on which a model was created.
          example: 2023-08-04T19:22:45.499127Z
          format: date-time
          type: string
        response:
          description: The response for a given prompt with a provided model.
          example: The sky appears blue because of a phenomenon called Rayleigh scattering.
          type: string
        done:
          description: Whether the response has completed.
          example: true
          type: boolean
        context:
          description: |
            An encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory.
          example:
          - 1
          - 2
          - 3
          items:
            type: integer
          type: array
        total_duration:
          description: Time spent generating the response.
          example: 5589157167
          type: number
        load_duration:
          description: Time spent in nanoseconds loading the model.
          example: 3013701500
          type: number
        prompt_eval_count:
          description: Number of tokens in the prompt.
          example: 46
          type: number
        prompt_eval_duration:
          description: Time spent in nanoseconds evaluating the prompt.
          example: 1160282000
          type: number
        eval_count:
          description: Number of tokens the response.
          example: 113
          type: number
        eval_duration:
          description: Time in nanoseconds spent generating the response.
          example: 1325948000
          type: number
      type: object
    GenerateChatCompletionRequest:
      description: Request class for the chat endpoint.
      example:
        stream: false
        format: null
        options:
          vocab_only: true
          seed: 1
          mirostat: 1
          f16_kv: true
          presence_penalty: 7.386282
          num_batch: 1
          penalize_newline: true
          top_p: 2.302136
          frequency_penalty: 1.2315135
          top_k: 5
          temperature: 2.027123
          use_mmap: true
          repeat_last_n: 3
          mirostat_eta: 6.846853
          main_gpu: 9
          logits_all: true
          num_thread: 8
          low_vram: true
          numa: true
          num_predict: 5
          rope_frequency_base: 9.36931
          tfs_z: 7.0614014
          num_ctx: 7
          num_gpu: 5
          use_mlock: true
          stop:
          - stop
          - stop
          mirostat_tau: 1.4894159
          repeat_penalty: 4.145608
          embedding_only: true
          num_keep: 6
          typical_p: 9.301444
          rope_frequency_scale: 6.6835623
          num_gqa: 4
        messages:
        - images:
          - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          role: system
          content: Why is the sky blue?
        - images:
          - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
          role: system
          content: Why is the sky blue?
        model: llama2:7b
      properties:
        model:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        messages:
          description: "The messages of the chat, this can be used to keep a chat\
            \ memory"
          items:
            $ref: '#/components/schemas/Message'
          type: array
        format:
          $ref: '#/components/schemas/ResponseFormat'
        options:
          $ref: '#/components/schemas/RequestOptions'
        stream:
          default: false
          description: |
            If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.
          type: boolean
      required:
      - messages
      - model
      type: object
    GenerateChatCompletionResponse:
      description: The response class for the chat endpoint.
      properties:
        message:
          $ref: '#/components/schemas/Message'
        model:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        created_at:
          description: Date on which a model was created.
          example: 2023-08-04T19:22:45.499127Z
          format: date-time
          type: string
        done:
          description: Whether the response has completed.
          example: true
          type: boolean
        total_duration:
          description: Time spent generating the response.
          example: 5589157167
          type: number
        load_duration:
          description: Time spent in nanoseconds loading the model.
          example: 3013701500
          type: number
        prompt_eval_count:
          description: Number of tokens in the prompt.
          example: 46
          type: number
        prompt_eval_duration:
          description: Time spent in nanoseconds evaluating the prompt.
          example: 1160282000
          type: number
        eval_count:
          description: Number of tokens the response.
          example: 113
          type: number
        eval_duration:
          description: Time in nanoseconds spent generating the response.
          example: 1325948000
          type: number
      type: object
    Message:
      description: A message in the chat endpoint
      example:
        images:
        - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
        - iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
        role: system
        content: Why is the sky blue?
      properties:
        role:
          description: The role of the message
          enum:
          - system
          - user
          - assistant
          type: string
        content:
          description: The content of the message
          example: Why is the sky blue?
          type: string
        images:
          description: (optional) a list of Base64-encoded images to include in the
            message (for multimodal models such as llava)
          items:
            description: Base64-encoded image (for multimodal models such as llava)
            example: iVBORw0KGgoAAAANSUhEUgAAAAkAAAANCAIAAAD0YtNRAAAABnRSTlMA/AD+APzoM1ogAAAAWklEQVR4AWP48+8PLkR7uUdzcMvtU8EhdykHKAciEXL3pvw5FQIURaBDJkARoDhY3zEXiCgCHbNBmAlUiyaBkENoxZSDWnOtBmoAQu7TnT+3WuDOA7KBIkAGAGwiNeqjusp/AAAAAElFTkSuQmCC
            type: string
          type: array
      required:
      - content
      - role
      type: object
    GenerateEmbeddingRequest:
      description: Generate embeddings from a model.
      example:
        options:
          vocab_only: true
          seed: 1
          mirostat: 1
          f16_kv: true
          presence_penalty: 7.386282
          num_batch: 1
          penalize_newline: true
          top_p: 2.302136
          frequency_penalty: 1.2315135
          top_k: 5
          temperature: 2.027123
          use_mmap: true
          repeat_last_n: 3
          mirostat_eta: 6.846853
          main_gpu: 9
          logits_all: true
          num_thread: 8
          low_vram: true
          numa: true
          num_predict: 5
          rope_frequency_base: 9.36931
          tfs_z: 7.0614014
          num_ctx: 7
          num_gpu: 5
          use_mlock: true
          stop:
          - stop
          - stop
          mirostat_tau: 1.4894159
          repeat_penalty: 4.145608
          embedding_only: true
          num_keep: 6
          typical_p: 9.301444
          rope_frequency_scale: 6.6835623
          num_gqa: 4
        model: llama2:7b
        prompt: Here is an article about llamas...
      properties:
        model:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        prompt:
          description: Text to generate embeddings for.
          example: Here is an article about llamas...
          type: string
        options:
          $ref: '#/components/schemas/RequestOptions'
      required:
      - model
      - prompt
      type: object
    GenerateEmbeddingResponse:
      description: Returns the embedding information.
      example:
        embedding:
        - 0.5670403838157654
        - 0.009260174818336964
        - '...'
      properties:
        embedding:
          description: The embedding for the prompt.
          example:
          - 0.5670403838157654
          - 0.009260174818336964
          - '...'
          items:
            format: double
            type: number
          type: array
      type: object
    CreateModelRequest:
      description: Create model request object.
      example:
        modelfile: FROM llama2\nSYSTEM You are mario from Super Mario Bros.
        stream: false
        name: mario
      properties:
        name:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: mario
          type: string
        modelfile:
          description: The contents of the Modelfile.
          example: FROM llama2\nSYSTEM You are mario from Super Mario Bros.
          type: string
        stream:
          default: false
          description: |
            If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.
          type: boolean
      required:
      - modelfile
      - name
      type: object
    CreateModelResponse:
      description: "Response object for creating a model. When finished, `status`\
        \ is `success`."
      properties:
        status:
          $ref: '#/components/schemas/CreateModelStatus'
      type: object
    CreateModelStatus:
      description: Status creating the model
      enum:
      - creating system layer
      - parsing modelfile
      - success
      type: string
    ModelsResponse:
      description: Response class for the list models endpoint.
      example:
        models:
        - size: 7323310500
          name: llama2:7b
          modified_at: 2023-08-02T17:02:23.713454393-07:00
        - size: 7323310500
          name: llama2:7b
          modified_at: 2023-08-02T17:02:23.713454393-07:00
      properties:
        models:
          description: List of models available locally.
          items:
            $ref: '#/components/schemas/Model'
          type: array
      type: object
    Model:
      description: A model available locally.
      example:
        size: 7323310500
        name: llama2:7b
        modified_at: 2023-08-02T17:02:23.713454393-07:00
      properties:
        name:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        modified_at:
          description: Model modification date.
          example: 2023-08-02T17:02:23.713454393-07:00
          format: date-time
          type: string
        size:
          description: Size of the model on disk.
          example: 7323310500
          type: integer
      type: object
    ModelInfoRequest:
      description: Request class for the show model info endpoint.
      example:
        name: llama2:7b
      properties:
        name:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
      required:
      - name
      type: object
    ModelInfo:
      description: "Details about a model including modelfile, template, parameters,\
        \ license, and system prompt."
      example:
        template: "[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\\\
          n\\n{{ end }}{{ .Prompt }} [/INST]"
        license: <contents of license block>
        modelfile: "Modelfile generated by \\\"ollama show\\\"\\n# To build a new\
          \ Modelfile based on this one, replace the FROM line with:\\n# FROM llama2:latest\\\
          n\\nFROM /Users/username/.ollama/models/blobs/sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8\\\
          nTEMPLATE \\\"\\\"\\\"[INST] {{ if and .First .System }}<<SYS>>{{ .System\
          \ }}<</SYS>>\\n\\n{{ end }}{{ .Prompt }} [/INST] \\\"\\\"\\\"\\nSYSTEM \\\
          \"\\\"\\\"\\\"\\\"\\\"\\nPARAMETER stop [INST]\\nPARAMETER stop [/INST]\\\
          nPARAMETER stop <<SYS>>\\nPARAMETER stop <</SYS>>\\n\""
        parameters: "stop [INST]\\nstop [/INST]\\nstop <<SYS>>\\nstop <</SYS>>"
      properties:
        license:
          description: The model's license.
          example: <contents of license block>
          type: string
        modelfile:
          description: The modelfile associated with the model.
          example: "Modelfile generated by \\\"ollama show\\\"\\n# To build a new\
            \ Modelfile based on this one, replace the FROM line with:\\n# FROM llama2:latest\\\
            n\\nFROM /Users/username/.ollama/models/blobs/sha256:8daa9615cce30c259a9555b1cc250d461d1bc69980a274b44d7eda0be78076d8\\\
            nTEMPLATE \\\"\\\"\\\"[INST] {{ if and .First .System }}<<SYS>>{{ .System\
            \ }}<</SYS>>\\n\\n{{ end }}{{ .Prompt }} [/INST] \\\"\\\"\\\"\\nSYSTEM\
            \ \\\"\\\"\\\"\\\"\\\"\\\"\\nPARAMETER stop [INST]\\nPARAMETER stop [/INST]\\\
            nPARAMETER stop <<SYS>>\\nPARAMETER stop <</SYS>>\\n\""
          type: string
        parameters:
          description: The model parameters.
          example: "stop [INST]\\nstop [/INST]\\nstop <<SYS>>\\nstop <</SYS>>"
          type: string
        template:
          description: The prompt template for the model.
          example: "[INST] {{ if and .First .System }}<<SYS>>{{ .System }}<</SYS>>\\\
            n\\n{{ end }}{{ .Prompt }} [/INST]"
          type: string
      type: object
    CopyModelRequest:
      description: Request class for copying a model.
      example:
        destination: llama2-backup
        source: llama2:7b
      properties:
        source:
          description: Name of the model to copy.
          example: llama2:7b
          type: string
        destination:
          description: Name of the new model.
          example: llama2-backup
          type: string
      required:
      - destination
      - source
      type: object
    DeleteModelRequest:
      description: Request class for deleting a model.
      example:
        name: llama2:13b
      properties:
        name:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:13b
          type: string
      required:
      - name
      type: object
    PullModelRequest:
      description: Request class for pulling a model.
      example:
        stream: false
        name: llama2:7b
        insecure: false
      properties:
        name:
          description: "The model name. \n\nModel names follow a `model:tag` format.\
            \ Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional\
            \ and, if not provided, will default to `latest`. The tag is used to identify\
            \ a specific version.\n"
          example: llama2:7b
          type: string
        insecure:
          default: false
          description: "Allow insecure connections to the library. \n\nOnly use this\
            \ if you are pulling from your own library during development.\n"
          type: boolean
        stream:
          default: false
          description: |
            If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.
          type: boolean
      required:
      - name
      type: object
    PullModelResponse:
      description: "Response class for pulling a model. \n\nThe first object is the\
        \ manifest. Then there is a series of downloading responses. Until any of\
        \ the download is completed, the `completed` key may not be included. \n\n\
        The number of files to be downloaded depends on the number of layers specified\
        \ in the manifest.\n"
      example:
        total: 2142590208
        digest: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        completed: 2142590208
        status: pulling manifest
      properties:
        status:
          $ref: '#/components/schemas/PullModelStatus'
        digest:
          description: The model's digest.
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
          type: string
        total:
          description: Total size of the model.
          example: 2142590208
          type: integer
        completed:
          description: Total bytes transferred.
          example: 2142590208
          type: integer
      type: object
    PullModelStatus:
      description: Status pulling the model.
      enum:
      - pulling manifest
      - downloading digestname
      - verifying sha256 digest
      - writing manifest
      - removing any unused layers
      - success
      example: pulling manifest
      type: string
    PushModelRequest:
      description: Request class for pushing a model.
      example:
        stream: false
        name: mattw/pygmalion:latest
        insecure: false
      properties:
        name:
          description: The name of the model to push in the form of <namespace>/<model>:<tag>.
          example: mattw/pygmalion:latest
          type: string
        insecure:
          default: false
          description: "Allow insecure connections to the library. \n\nOnly use this\
            \ if you are pushing to your library during development.\n"
          type: boolean
        stream:
          default: false
          description: |
            If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects.
          type: boolean
      required:
      - name
      type: object
    PushModelResponse:
      description: Response class for pushing a model.
      example:
        total: 2142590208
        digest: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
        status: null
      properties:
        status:
          $ref: '#/components/schemas/PushModelStatus'
        digest:
          description: the model's digest
          example: sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711a
          type: string
        total:
          description: total size of the model
          example: 2142590208
          type: integer
      type: object
    PushModelStatus:
      description: Status pushing the model.
      enum:
      - retrieving manifest
      - starting upload
      - pushing manifest
      - success
      type: string

